{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dae9986-c24b-43a8-9da4-4be8032d08c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "\n",
    "from subprocess import run, DEVNULL\n",
    "\n",
    "from torch.nn import Sequential, Sigmoid\n",
    "from torch import from_numpy, inference_mode\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from zipfile import BadZipFile\n",
    "\n",
    "import xmltodict\n",
    "\n",
    "import os\n",
    "from os.path import join, exists, getsize, isfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b686fae5-e348-4846-b236-2aae771ae8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PedroNet(LightningModule): #out_channels = numero de classes\n",
    "    def __init__(self, img_size, lr,\n",
    "                 depths=(2, 2, 2, 2), \n",
    "                 num_heads=(3, 6, 12, 24), \n",
    "                 feature_size=24, \n",
    "                 norm_name='instance', \n",
    "                 drop_rate=0.0, \n",
    "                 attn_drop_rate=0.0, \n",
    "                 dropout_path_rate=0.0, \n",
    "                 normalize=True, \n",
    "                 use_checkpoint=False, \n",
    "                 downsample='merging', \n",
    "                 use_v2=False \n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.model = Sequential(SwinUNETR(spatial_dims=2,\n",
    "                                    in_channels=1,\n",
    "                                    out_channels=1,\n",
    "                                    depths=depths,\n",
    "                                    img_size=img_size,\n",
    "                                    feature_size=feature_size,\n",
    "                                    drop_rate=drop_rate,\n",
    "                                    num_heads=num_heads,\n",
    "                                    norm_name=norm_name,\n",
    "                                    attn_drop_rate=attn_drop_rate,\n",
    "                                    dropout_path_rate=dropout_path_rate,\n",
    "                                    normalize=normalize,\n",
    "                                    use_checkpoint=use_checkpoint,\n",
    "                                    downsample=downsample,\n",
    "                                    #use_v2=use_v2, apenas para versões mais recentes\n",
    "                                    ))\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "class SarAcquisition:\n",
    "    def __init__(self, email, \n",
    "                 password,\n",
    "                 raw_name_list,\n",
    "                 satellite=\"sentinel_1\", \n",
    "                 raw_folder=None, \n",
    "                 netcdf_folder=None,\n",
    "                 unzip_folder=None, \n",
    "                 img_folder=None, \n",
    "                 label_folder=None,\n",
    "                 auto_labels=None,\n",
    "                 unzip_file=True):\n",
    "\n",
    "        self.password = password\n",
    "        self.email = email\n",
    "        self.raw_name_list = raw_name_list\n",
    "        self.satellite = satellite\n",
    "        self.unzip_file = unzip_file\n",
    "        self.raw_folder = self._create_folder(raw_folder, \"raw_folder\")\n",
    "        self.img_folder = self._create_folder(img_folder, \"img_folder\")\n",
    "        self.netcdf_folder = self._create_folder(netcdf_folder, \"netcdf_folder\")\n",
    "        self.unzip_folder = self._create_folder(unzip_folder, \"unzip_folder\")\n",
    "        self.auto_labels = self._create_folder(auto_labels, \"auto_labels\")\n",
    "        self._query_result = None\n",
    "\n",
    "        self.all_folders = {\n",
    "            \"raw_folder\": self.raw_folder,\n",
    "            #\"img_folder\": self.img_folder,\n",
    "            \"netcdf_folder\": self.netcdf_folder,\n",
    "            \"unzip_folder\": self.unzip_folder\n",
    "        }      \n",
    "\n",
    "    def _create_folder(self, folder_path, original_name):\n",
    "        if not folder_path:\n",
    "            folder_path = join(os.getcwd(), original_name)\n",
    "            if not exists(folder_path):\n",
    "                os.mkdir(folder_path)\n",
    "\n",
    "        assert exists(folder_path), f\"Path not found: {folder_path}\"\n",
    "        return folder_path\n",
    "    \n",
    "    def is_downloaded(self, sar_name):\n",
    "        for folder in self.all_folders.values():\n",
    "            sar_path = join(folder, sar_name)\n",
    "            if exists(sar_path):\n",
    "                #if getsize(sar_path) > 5E9: #ver o tamanho do arquivo para baixar arquivos incompletos\n",
    "                return True\n",
    "        return False     \n",
    "\n",
    "    def query_result(self):\n",
    "        if not isinstance(self._query_result, pd.DataFrame):\n",
    "            self._query_result = self.query_sar()\n",
    "        return self._query_result\n",
    "    \n",
    "    def query_sar(self):\n",
    "        API_URL_NAME = \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=contains(Name,'{name}')\"\n",
    "        query_result = pd.DataFrame()\n",
    "\n",
    "        for product in self.raw_name_list:\n",
    "            json = requests.get(API_URL_NAME.format(name=product)).json()\n",
    "            response_result = pd.DataFrame.from_dict(json['value'])\n",
    "\n",
    "            if not response_result.empty:\n",
    "                print(\"Found:\", product)\n",
    "                query_result = pd.concat([query_result, response_result])   \n",
    "                #yield response_result\n",
    "            else:\n",
    "                print(\"Could not find: \", product)\n",
    "\n",
    "\n",
    "        #columns_to_print = ['Id', 'Name','GeoFootprint']  \n",
    "        #display(query_result[columns_to_print])\n",
    "        return query_result\n",
    "\n",
    "    def return_headers(self):\n",
    "        token_url = 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token'\n",
    "        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "        data = {\n",
    "            'grant_type': 'password',\n",
    "            'username': self.email,\n",
    "            'password': self.password,\n",
    "            'client_id': 'cdse-public'\n",
    "        }\n",
    "\n",
    "        token_response = requests.post(token_url, headers=headers, data=data).json()\n",
    "        token_url = token_response[\"access_token\"]\n",
    "        download_headers = {\"Authorization\": f\"Bearer {token_url}\"}\n",
    "        return download_headers\n",
    "    \n",
    "    def download_product(self, product_name, product_id, overwrite=False):\n",
    "        download_url = \"https://zipper.dataspace.copernicus.eu/odata/v1/Products({product_id})/$value\"\n",
    "\n",
    "        path_product = join(self.raw_folder, product_name + \".zip\")\n",
    "        if not self.is_downloaded(product_name) or overwrite:\n",
    "            session = requests.Session()\n",
    "            session.headers.update(self.return_headers())\n",
    "            response = session.get(download_url.format(product_id=product_id), headers=self.return_headers(), stream=True)\n",
    "            total_size = int(response.headers.get('Content-Length', 0))\n",
    "            progress_bar = tqdm(total=total_size, unit='B', unit_scale=True, desc=f'Downloading: {product_name}', leave=True)\n",
    "\n",
    "            with open(path_product, \"wb\") as file:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        file.write(chunk)\n",
    "                        progress_bar.update(len(chunk))\n",
    "            progress_bar.refresh()\n",
    "            progress_bar.reset()\n",
    "\n",
    "\n",
    "    def download_all_products(self):\n",
    "        for index, product_output in self.query_result().iterrows():\n",
    "            self.download_product(product_output[\"Name\"], product_output[\"Id\"])\n",
    "\n",
    "    def unzip_products(self):\n",
    "        bad_download = []\n",
    "        for product_zip in os.listdir(self.raw_folder):\n",
    "            product_zip_path = join(self.raw_folder, product_zip)\n",
    "            product_unzip_path = join(self.unzip_folder, product_zip[:-4]) \n",
    "            if not product_zip[:-4] in os.listdir(self.unzip_folder):\n",
    "                print(\"Unziping:\", product_zip)\n",
    "                try:\n",
    "                    with ZipFile(product_zip_path, 'r') as zip_ref:\n",
    "                        zip_ref.extractall(product_unzip_path)\n",
    "                except BadZipFile:\n",
    "                    print(f\"Imposible to unzip: {product_zip}; Adding to download queue\")\n",
    "                    bad_download.append(product_zip)\n",
    "        \n",
    "\n",
    "\n",
    "    def edit_graph_xml(self, graph_path, product_name):\n",
    "        #Abrindo o arquivo XML e transformando em um dicionário.\n",
    "        with open(graph_path) as arquivo:\n",
    "            dados = xmltodict.parse(arquivo.read())\n",
    "\n",
    "        if product_name in os.listdir(self.unzip_folder):\n",
    "            input_path = join(self.unzip_folder, product_name)\n",
    "        else:\n",
    "            input_path = join(self.raw_folder, product_name + \".zip\")\n",
    "\n",
    "        output_path = join(self.netcdf_folder, product_name[:-4] + \"nc\")\n",
    "        #Editando os pontos referentes as entradas e saidas do arquivo NetCDF.\n",
    "        dados['graph']['node'][0]['parameters']['file'] = input_path #Modificando o diretorio de entrada.\n",
    "        dados['graph']['node'][-1]['parameters']['file'] = output_path #Mesma coisa para saida.\n",
    "        #Salvando as alterações no arquivo XML, note que estamos sobrescrevendo o mesmo grafo.\n",
    "        with open(graph_path, 'w') as arquivo:\n",
    "            arquivo.write(xmltodict.unparse(dados, pretty=True))\n",
    "\n",
    "    def convert_netcdf4(self, graph_path, gpt_path):\n",
    "        for product_name in self.raw_name_list:\n",
    "            if not product_name in os.listdir(self.netcdf_folder):\n",
    "                self.edit_graph_xml(graph_path, product_name)\n",
    "                shell = run([gpt_path, graph_path])#, stdout=DEVNULL, stderr=DEVNULL)\n",
    "\n",
    "    \n",
    "    def create_png(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def create_oil_label(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def create_polygon_list(self, mask):\n",
    "        edited_contours = []\n",
    "        binary_image = np.array(mask)\n",
    "        \n",
    "        contours, hierarchy = cv2.findContours(binary_image.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for poly in contours:\n",
    "            if len(poly) > 50:\n",
    "                approx = cv2.approxPolyDP(poly, 0.8, True)\n",
    "                approx = np.squeeze(approx)\n",
    "                edited_contours.append(approx)\n",
    "        return edited_contours\n",
    "\n",
    "    def polygon_to_labelme(self, polygons, image_name):\n",
    "        labelme_format = {\"version\": \"5.1.1\",\n",
    "                          \"flags\": {},\n",
    "                          \"shapes\": [],\n",
    "                          \"imagePath\": f\"..\\\\Sar_img\\\\{image_name}.png\",  # Update with your image filename\n",
    "                          \"imageData\": None,\n",
    "                          \"imageHeight\": nc_img.shape[0],\n",
    "                          \"imageWidth\": nc_img.shape[1]\n",
    "                          }\n",
    "\n",
    "        for patch in polygons:\n",
    "            labelme_format[\"shapes\"].append({\n",
    "                \"label\": \"oil\", \n",
    "                \"points\": patch.squeeze().tolist(),\n",
    "                \"group_id\": None,\n",
    "                \"description\": \"\",\n",
    "                \"shape_type\": \"polygon\",\n",
    "                \"flags\": {}\n",
    "            })\n",
    "        \n",
    "        auto_labels_path = join(self.auto_labels, f\"{image_name}.json\")\n",
    "        with open(auto_labels_path, 'w') as json_file:\n",
    "            json.dump(labelme_format, json_file, indent=2)\n",
    "\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c462c27c-d25a-46b7-94a2-6cd1a90bcdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['S1A_IW_SLC__1SDV_20141004T154823_20141004T154851_002682_002FE4_C094.SAFE']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nc_img' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(query_sar)\n\u001b[1;32m     10\u001b[0m test \u001b[38;5;241m=\u001b[39m SarAcquisition(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpedro.meirelles@ufba.br\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThermal1234@\u001b[39m\u001b[38;5;124m\"\u001b[39m, raw_name_list\u001b[38;5;241m=\u001b[39mquery_sar)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolygon_to_labelme\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#test.unzip_products()\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#test.convert_netcdf4(SAR_TO_NC_GRAPH, PATH_TO_GPT)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 217\u001b[0m, in \u001b[0;36mSarAcquisition.polygon_to_labelme\u001b[0;34m(self, polygons, image_name)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpolygon_to_labelme\u001b[39m(\u001b[38;5;28mself\u001b[39m, polygons, image_name):\n\u001b[1;32m    212\u001b[0m     labelme_format \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5.1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    213\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflags\u001b[39m\u001b[38;5;124m\"\u001b[39m: {},\n\u001b[1;32m    214\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes\u001b[39m\u001b[38;5;124m\"\u001b[39m: [],\n\u001b[1;32m    215\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimagePath\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSar_img\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mimage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Update with your image filename\u001b[39;00m\n\u001b[1;32m    216\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimageData\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m--> 217\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimageHeight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mnc_img\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    218\u001b[0m                       \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimageWidth\u001b[39m\u001b[38;5;124m\"\u001b[39m: nc_img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    219\u001b[0m                       }\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m patch \u001b[38;5;129;01min\u001b[39;00m new_patches:\n\u001b[1;32m    222\u001b[0m         labelme_format[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend({\n\u001b[1;32m    223\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moil\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m    224\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m: patch\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mtolist(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflags\u001b[39m\u001b[38;5;124m\"\u001b[39m: {}\n\u001b[1;32m    229\u001b[0m         })\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nc_img' is not defined"
     ]
    }
   ],
   "source": [
    "QUERY_CSV_PATH = \"/mnt/camobi_2/PHMG/new_img_acqusition/New_sar_img.csv\"\n",
    "FILE_NAME_COLUMN = \"sar_file_name\"\n",
    "SAR_TO_NC_GRAPH = \"/mnt/camobi_2/PHMG/new_img_acqusition/Grafos/ZIP_to_NC.xml\"\n",
    "PATH_TO_GPT = \"/home/camobi/snap/bin/gpt\"\n",
    "query_sar = pd.read_csv(QUERY_CSV_PATH, header=0)[FILE_NAME_COLUMN]\n",
    "query_sar = [query_sar[0]]\n",
    "print(query_sar)\n",
    "\n",
    "\n",
    "test = SarAcquisition(\"pedro.meirelles@ufba.br\", \"Thermal1234@\", raw_name_list=query_sar)\n",
    "test.polygon_to_labelme([1,2,3,4,5,6,7,8], \"test\")\n",
    "#test.unzip_products()\n",
    "\n",
    "\n",
    "\n",
    "#test.convert_netcdf4(SAR_TO_NC_GRAPH, PATH_TO_GPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ac0293-fff9-4ba7-9903-6cfa6d239e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = PedroNet.load_from_checkpoint(WEIGHTS_MODEL)\n",
    "loaded_model = DataParallel(loaded_model)\n",
    "loaded_model = loaded_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0a0b5fc-db1a-4aaa-93bd-1d9b815f0f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: S1A_IW_SLC__1SDV_20141004T154823_20141004T154851_002682_002FE4_C094.SAFE:   0%|          | 5.31M/8.69G [01:24<38:12:47, 63.1kB/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/mnt/camobi_2/PHMG/new_img_acqusition/raw_folder/S1A_IW_SLC__1SDV_20230505T214735_20230505T214802_048404_05D283_FAB0.SAFE.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m path_test \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/camobi_2/PHMG/new_img_acqusition/raw_folder/S1A_IW_SLC__1SDV_20230505T214735_20230505T214802_048404_05D283_FAB0.SAFE.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/camobi_2/PHMG/new_img_acqusition/unzip_folder/S1A_IW_SLC__1SDV_20230505T214735_20230505T214802_048404_05D283_FAB0.SAFE\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ZipFile(path_test, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m      4\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(output)\n",
      "File \u001b[0;32m/home/camobi/anaconda3/envs/torch/lib/python3.11/zipfile.py:1284\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1282\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1284\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mopen(file, filemode)\n\u001b[1;32m   1285\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/mnt/camobi_2/PHMG/new_img_acqusition/raw_folder/S1A_IW_SLC__1SDV_20230505T214735_20230505T214802_048404_05D283_FAB0.SAFE.zip'"
     ]
    }
   ],
   "source": [
    "path_test = (\"/mnt/camobi_2/PHMG/new_img_acqusition/raw_folder/S1A_IW_SLC__1SDV_20230505T214735_20230505T214802_048404_05D283_FAB0.SAFE.zip\")\n",
    "output = \"/mnt/camobi_2/PHMG/new_img_acqusition/unzip_folder/S1A_IW_SLC__1SDV_20230505T214735_20230505T214802_048404_05D283_FAB0.SAFE\"\n",
    "with ZipFile(path_test, 'r') as zip_ref:\n",
    "    zip_ref.extractall(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4329d503-01fd-4334-a555-bdc3f6eba276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
