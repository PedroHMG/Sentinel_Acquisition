{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7750d52-afa6-433f-a3bf-2778a5d826e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import netCDF4 as nc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from subprocess import run, DEVNULL\n",
    "\n",
    "from torch.nn import Sequential, Sigmoid\n",
    "from torch import from_numpy, inference_mode\n",
    "from torch.nn import DataParallel\n",
    "\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai.inferers import sliding_window_inference\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from zipfile import BadZipFile\n",
    "\n",
    "import xmltodict\n",
    "\n",
    "import os\n",
    "from os.path import join, exists, getsize, isfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbcc5ce-3210-4c54-af10-6e3bc138c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownloadSARSentinel:\n",
    "    def __init__(self, email, \n",
    "             password,\n",
    "             product_list,\n",
    "             satellite=\"sentinel_1\"\n",
    "                ):\n",
    "        self.password = password\n",
    "        self.email = email\n",
    "        self.product_list = product_list\n",
    "        self.satellite = satellite\n",
    "        self.raw_folder = None\n",
    "        self._query_result = None\n",
    "\n",
    "    def query_result(self):\n",
    "        if not isinstance(self._query_result, pd.DataFrame):\n",
    "            self._query_result = self.query_sar()\n",
    "        return self._query_result\n",
    "        \n",
    "    def query_sar(self):\n",
    "        API_URL_NAME = \"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?$filter=contains(Name,'{name}')\"\n",
    "        query_result = pd.DataFrame()\n",
    "\n",
    "        for product in self.product_list:\n",
    "            json = requests.get(API_URL_NAME.format(name=product)).json()\n",
    "            response_result = pd.DataFrame.from_dict(json['value'])\n",
    "\n",
    "            if not response_result.empty:\n",
    "                print(\"Found:\", product)\n",
    "                query_result = pd.concat([query_result, response_result])   \n",
    "                #yield response_result\n",
    "            else:\n",
    "                print(\"Could not find: \", product)\n",
    "\n",
    "        return query_result\n",
    "        \n",
    "    @staticmethod\n",
    "    def create_folder(folder_path, original_name):\n",
    "        if not folder_path:\n",
    "            folder_path = join(os.getcwd(), original_name)\n",
    "            if not exists(folder_path):\n",
    "                os.mkdir(folder_path)\n",
    "\n",
    "        assert exists(folder_path), f\"Path not found: {folder_path}\"\n",
    "        return folder_path\n",
    "\n",
    "    @staticmethod\n",
    "    def is_downloaded(sar_name, folder):\n",
    "        sar_path = join(folder, sar_name + \".zip\")\n",
    "        if exists(sar_path):\n",
    "            #if getsize(sar_path) > 5E9: #ver o tamanho do arquivo para baixar arquivos incompletos\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def return_headers(self):\n",
    "        token_url = 'https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token'\n",
    "        headers = {'Content-Type': 'application/x-www-form-urlencoded'}\n",
    "        data = {\n",
    "            'grant_type': 'password',\n",
    "            'username': self.email,\n",
    "            'password': self.password,\n",
    "            'client_id': 'cdse-public'\n",
    "        }\n",
    "        token_response = requests.post(token_url, headers=headers, data=data).json()\n",
    "        token_url = token_response[\"access_token\"]\n",
    "        download_headers = {\"Authorization\": f\"Bearer {token_url}\"}\n",
    "        return download_headers\n",
    "\n",
    "    def download_products(self, folder, overwrite=False):\n",
    "        download_url = \"https://zipper.dataspace.copernicus.eu/odata/v1/Products({product_id})/$value\"\n",
    "\n",
    "        for index, product in self.query_result().iterrows():\n",
    "            path_product = join(folder, product[\"Name\"] + \".zip\")               \n",
    "            if not DownloadSARSentinel.is_downloaded(product[\"Name\"], folder) or overwrite:\n",
    "                session = requests.Session()\n",
    "                session.headers.update(self.return_headers())\n",
    "                response = session.get(download_url.format(product_id=product[\"Id\"]), headers=self.return_headers(), stream=True)\n",
    "                total_size = int(response.headers.get('Content-Length', 0))\n",
    "                progress_bar = tqdm(total=total_size, unit='B', unit_scale=True, desc=f'Downloading: {product[\"Name\"]}', leave=True)\n",
    "    \n",
    "                with open(path_product, \"wb\") as file:\n",
    "                    for chunk in response.iter_content(chunk_size=8192):\n",
    "                        if chunk:\n",
    "                            file.write(chunk)\n",
    "                            progress_bar.update(len(chunk))\n",
    "                progress_bar.reset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f851afcb-b8ac-458c-b45b-ac664df60343",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_CSV_PATH = \"/mnt/camobi_2/PHMG/Sentinel_Acquisition/New_sar_img.csv\"\n",
    "FILE_NAME_COLUMN = \"sar_file_name\"\n",
    "query_sar = pd.read_csv(QUERY_CSV_PATH, header=0)[FILE_NAME_COLUMN]\n",
    "query_sar = query_sar[2:4]\n",
    "#print(query_sar)\n",
    "\n",
    "test = DownloadSARSentinel(\"pedro.meirelles@ufba.br\", \"Thermal1234@\", product_list=list(query_sar))\n",
    "\n",
    "test.query_result()\n",
    "test.download_products(folder=\"/mnt/camobi_2/PHMG/Sentinel_Acquisition/raw_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f1b973-297a-41d8-81a6-c2383367f35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentinelProduct:\n",
    "    def __init__(self, name, file_path, nc_graph_path,  rect_corners=None):\n",
    "        self.name = name\n",
    "        self.rect_corners = rect_corners\n",
    "        self.file_path = file_path\n",
    "        self.nc_graph_path = nc_graph_path\n",
    "        self.unzip_path = None\n",
    "        self.netcdf_path = None\n",
    "\n",
    "    def unzip(self, unzip_folder):\n",
    "        assert exists(unzip_folder), f\"Folder \\\"{unzip_folder}\\\" does not exist!\"\n",
    "        self.unzip_path = join(unzip_folder, self.name) \n",
    "\n",
    "        try:\n",
    "            print(\"Unziping:\", self.name)\n",
    "            with ZipFile(self.file_path, 'r') as zip_ref:\n",
    "                zip_ref.extractall(self.unzip_path)\n",
    "        except BadZipFile:\n",
    "            print(f\"Imposible to unzip: {self.name}. file is incomplete or corrupted!\")\n",
    "\n",
    "    def edit_zip_to_nc(self, netcdf_folder):\n",
    "        with open(self.nc_graph_path) as arquivo:\n",
    "            dados = xmltodict.parse(arquivo.read())\n",
    "\n",
    "        if not self.unzip_path:\n",
    "            input_path = self.unzip_path\n",
    "        else:\n",
    "            input_path = self.file_path\n",
    "\n",
    "        self.netcdf_path = join(netcdf_folder, self.name + \".nc\")\n",
    "\n",
    "        dados['graph']['node'][0]['parameters']['file'] = input_path \n",
    "        dados['graph']['node'][-1]['parameters']['file'] = self.netcdf_path \n",
    "\n",
    "        with open(self.nc_graph_path, 'w') as arquivo:\n",
    "            arquivo.write(xmltodict.unparse(dados, pretty=True))\n",
    "\n",
    "    def convert_to_netcdf4(self, gpt_path, netcdf_folder):\n",
    "        self.edit_zip_to_nc(netcdf_folder)\n",
    "        shell = run([gpt_path, self.nc_graph_path])#, stdout=DEVNULL, stderr=DEVNULL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9edfa62-0ed5-4e8a-b021-3ae592019e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAR_TO_NC_GRAPH = \"/mnt/camobi_2/PHMG/Sentinel_Acquisition/graphs/ZIP_to_NC.xml\"\n",
    "PATH_TO_GPT = \"/home/camobi/snap/bin/gpt\"\n",
    "NETCDF_FOLER = \"/mnt/camobi_2/PHMG/Sentinel_Acquisition/netcdf_folder\"\n",
    "UNZIP_FOLDER = \"/mnt/camobi_2/PHMG/Sentinel_Acquisition/unzip_folder\"\n",
    "\n",
    "\n",
    "product_name = pd.read_csv(QUERY_CSV_PATH, header=0)[FILE_NAME_COLUMN][0][:-5]\n",
    "product_path = join(\"/mnt/camobi_2/PHMG/Sentinel_Acquisition/raw_folder\", product_name + \".SAFE.zip\")\n",
    "\n",
    "product = SentinelProduct(product_name, product_path, SAR_TO_NC_GRAPH)\n",
    "product.unzip(UNZIP_FOLDER)\n",
    "product.convert_to_netcdf4(PATH_TO_GPT, NETCDF_FOLER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf87e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PedroNet(LightningModule): #out_channels = numero de classes\n",
    "    def __init__(self, img_size, lr,\n",
    "                 depths=(2, 2, 2, 2), \n",
    "                 num_heads=(3, 6, 12, 24), \n",
    "                 feature_size=24, \n",
    "                 norm_name='instance', \n",
    "                 drop_rate=0.0, \n",
    "                 attn_drop_rate=0.0, \n",
    "                 dropout_path_rate=0.0, \n",
    "                 normalize=True, \n",
    "                 use_checkpoint=False, \n",
    "                 downsample='merging', \n",
    "                 use_v2=False \n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.model = Sequential(SwinUNETR(spatial_dims=2,\n",
    "                                    in_channels=1,\n",
    "                                    out_channels=1,\n",
    "                                    depths=depths,\n",
    "                                    img_size=img_size,\n",
    "                                    feature_size=feature_size,\n",
    "                                    drop_rate=drop_rate,\n",
    "                                    num_heads=num_heads,\n",
    "                                    norm_name=norm_name,\n",
    "                                    attn_drop_rate=attn_drop_rate,\n",
    "                                    dropout_path_rate=dropout_path_rate,\n",
    "                                    normalize=normalize,\n",
    "                                    use_checkpoint=use_checkpoint,\n",
    "                                    downsample=downsample,\n",
    "                                    #use_v2=use_v2, apenas para versões mais recentes\n",
    "                                    ))\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def netcdf_inferece(self, nc_product):\n",
    "        torch_img = from_numpy(nc_product.nc_img).to(\"cuda\")\n",
    "        torch_img = torch_img.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "        with inference_mode():\n",
    "            model_img = sliding_window_inference(torch_img, \n",
    "                                            roi_size=(512),\n",
    "                                            sw_batch_size=20, \n",
    "                                            predictor=DataParallel(self.model), \n",
    "                                            mode='gaussian',\n",
    "                                            overlap=0.3,\n",
    "                                            progress=True\n",
    "                                            )\n",
    "\n",
    "            sigmoid_fn = Sigmoid()\n",
    "            model_img = sigmoid_fn(model_img)\n",
    "            model_img = model_img.to(\"cpu\")\n",
    "        return model_img\n",
    "\n",
    "class NetcdfProduct:\n",
    "    def __init__(self, product, image_variable='Sigma0_VV_db'):\n",
    "        self.product = product\n",
    "        self.name = product.filepath().split(\"/\")[-1].split(\".\")[0]\n",
    "        self.image_variable = image_variable\n",
    "        self.nc_img = np.asarray(product.variables[self.image_variable][:])\n",
    "\n",
    "    def create_img(self, folder_path):\n",
    "        img_path = join(folder_path, self.name + \".png\")\n",
    "        normalized_img = ((self.nc_img - np.min(self.nc_img)) / (np.max(self.nc_img) - np.min(self.nc_img))) * 255\n",
    "        cv2.imwrite(img_path, normalized_img)\n",
    "    \n",
    "    @staticmethod\n",
    "    def create_polygons(mask):\n",
    "        edited_contours = []\n",
    "        binary_image = np.array(mask)\n",
    "        \n",
    "        contours, hierarchy = cv2.findContours(binary_image.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        \n",
    "        for poly in contours:\n",
    "            if len(poly) > 50:\n",
    "                approx = cv2.approxPolyDP(poly, 0.8, True)\n",
    "                approx = np.squeeze(approx)\n",
    "                edited_contours.append(approx)\n",
    "        return edited_contours\n",
    "\n",
    "    def polygon_to_labelme(self, polygons, mask):\n",
    "        masked_polygons = NetcdfProduct.create_polygons(mask)\n",
    "        labelme_format = {\"version\": \"5.1.1\",\n",
    "                          \"flags\": {},\n",
    "                          \"shapes\": [],\n",
    "                          \"imagePath\": f\"..\\\\Sar_img\\\\{self.name}.png\",  # Update with your image filename\n",
    "                          \"imageData\": None,\n",
    "                          \"imageHeight\": self.product.dimensions[\"y\"],\n",
    "                          \"imageWidth\": self.product.dimensions[\"x\"]\n",
    "                          }\n",
    "\n",
    "        for patch in polygons:\n",
    "            labelme_format[\"shapes\"].append({\n",
    "                \"label\": \"oil\", \n",
    "                \"points\": patch.squeeze().tolist(),\n",
    "                \"group_id\": None,\n",
    "                \"description\": \"\",\n",
    "                \"shape_type\": \"polygon\",\n",
    "                \"flags\": {}\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5659c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NETCDF_PRODUCT_PATH = \"/mnt/camobi_process/new_data/images_nc/4A2D.nc\"\n",
    "\n",
    "nc_img = nc.Dataset(NETCDF_PRODUCT_PATH, 'r')\n",
    "netcdf_sar = NetcdfProduct(nc_img)\n",
    "netcdf_sar.product\n",
    "#netcdf_sar.create_img(\"/mnt/camobi_2/PHMG/Sentinel_Acquisition/img_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db19912",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS_MODEL = \"/mnt/camobi_2/PHMG/PedroSwinNet/Model_512Img_24Feature_(2, 2, 2, 2)depths_0.0attnDrop_(3, 6, 12, 24)Heads_30.000000Lr_0drop_v1/model-Val_loss=0.003714-Val_Precision=0.973-Recall=0.971-Val_F1_Score=0.972.ckpt\"\n",
    "loaded_model = PedroNet.load_from_checkpoint(WEIGHTS_MODEL)\n",
    "loaded_model.netcdf_inferece(netcdf_sar)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
